<!DOCTYPE html>
<!-- saved from url=(0041)file:///C:/Users/jench/Desktop/index.html -->
<html><head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-53775284-6"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-53775284-6');
</script>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<!--
<link rel="icon" href="https://nv-tlabs.github.io/images/logo_hu2fe6632db44d28c9b9d53edd3914c1d6_112452_0x70_resize_lanczos_2.png">
-->


</head>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }

    h1 {
        font-weight:300;
        margin: 0.4em;
    }

    p {
        margin: 0.2em;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
                15px 15px 0 0px #fff, /* The fourth layer */
                15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
                20px 20px 0 0px #fff, /* The fifth layer */
                20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
                25px 25px 0 0px #fff, /* The fifth layer */
                25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr
    {
        margin: 0;
        border: 0;
        height: 1.5px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
</style>

<html>
  <head>
        <title>MPS-Net</title>
  </head>

  <body>
    <br>
    <center>
    <span style="font-size:42px">Capturing Humans in Motion: 3D Human Pose and Shape Estimation Through Motion Continuity Attention and Hierarchical Attentive Feature Integration</span>
    </center>

    <br>
 <!--     <table align=center width=700px>
       <tr> 

                <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="https://www.alexyuxuanzhang.com/">Yuxuan Zhang *</a><sup>1,4</sup></span>
        </center>
        </td>




        <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="http://www.cs.toronto.edu/~linghuan/">Huan Ling *</a><sup>1,2,3</sup></span>
        </center>
        </td>



        <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="http://www.cs.toronto.edu/~jungao/">Jun Gao </a><sup>1,2,3</sup></span>
        </center>
        </td>

        <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="https://kangxue.org/">Kangxue Yin </a><sup>1</sup></span>
        </center>
        </td>



       
     </tr>
    </table>


      <table align=center width=700px>

    <tr>
              <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="">Jean-Francois Lafleche</a><sup>1</sup></span>
        </center>
        </td>

        <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="">Adela Barriuso</a><sup>5</sup></span>
        </center>
        </td>
                <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="https://groups.csail.mit.edu/vision/torralbalab/"> Antonio Torralba</a><sup>5</sup></span>
        </center>
        </td>

      <td align=center width=100px>
            <center>
            <span style="font-size:20px"><a href="http://www.cs.toronto.edu/~fidler/">Sanja Fidler</a><sup>1,2,3</sup></span>
            </center>
        </td>

       
     </tr>
    </table>
-->


    <br>
<!--
    <table align=center width=700px>
       <tr>
              <td align=center width=100px>
            <center>
            <span style="font-size:20px"><sup>1</sup>NVIDIA</span>
            </center>
        </td>


        <td align=center width=100px>
        <center>

        <span style="font-size:20px"><sup>2</sup>University of Toronto</span>
        </center>
        </td>
        <td align=center width=100px>
            <center>
            <span style="font-size:20px"><sup>3</sup>Vector Institute</span>
            </center>
        </td>
                <td align=center width=100px>
            <center>
            <span style="font-size:20px"><sup>4</sup>University of Waterloo</span>
            </center>
        </td>
    
     <td align=center width=100px>
            <center>
            <span style="font-size:20px"><sup>5</sup>MIT</span>
            </center>
        </td>
     </tr>
    </table>
-->    
    <table align=center width=700px>
       <tr>
        <td align=center width=100px>
  
        </td>
     </tr>
    </table>

            <br>
            <table align=center width=900px>
                        <tr>    
                      <center>
                       <br>
                    </center>
                   
                    </td>
               
                      
                <tr>
                        <td width=600px>

                      <center>

                       
                     
                             <img src = "./Fig3.png" width="900px" height="410px"></img>
                                                 <span style="font-size:15px;font-style:italic">   Overview of our motion pose and shape network (MPS-Net). MPS-Net estimates pose, shape, and camera parameters in the
video sequence based on the static feature extractor, temporal encoder, temporal feature integration scheme, and SMPL parameter regressor to generate 3D human pose and shape.</span>

                    </center>

                    </td>
                         </tr>
           
                </tr>
            </table>
            <table align=center width=900px></table>

                <tr>
                    <td width=600px>
                        <br>
                        <p align="justify" style="font-size: 18px">
                         Learning to capture human motion is essential to 3D human pose and shape estimation from monocular video. However, the existing methods mainly rely on recurrent or convolutional operation to model such temporal information, which limits the ability to capture non-local context relations of 3D human motion. To address this problem, we propose a motion pose and shape network (MPS-Net) to effectively capture humans in motion to generate accurate and temporally coherent 3D human pose and shape from a video. Specifically, we first propose a motion continuity attention (MoCA) module that leverages visual cues observed from human motion to adaptively recalibrate the
range that needs attention in the sequence to better capture the motion continuity dependencies. We then propose a hierarchical attentive feature integration (HAFI) module to effectively combine past and future feature representations to strengthen temporal correlation and refine the feature representation of the current frame. By jointly considering MoCA and HAFI modules, the proposed MPS-Net excels in estimating 3D human pose and shape in the video. Though conceptually simple, our MPS-Net outperforms the state-of-the-art methods on the challenging 3DPW, MPI-INF-3DHP and Human3.6M benchmark datasets, respectively.
                        </p> </td>

                             <center>
                              <td width=600px>
                         <img src = "./resources/360.mp4" width="450px" height="253px"></img>
                         <img src = "./resources/interp.gif" width="230px" height="250px"></img><br>
                                    <span style="font-size:14px">   Left: The video showcases our detailed part segmentation in reconstructing animatable 3D objects from monocular
images. Right: The video shows the result of running interpolation over latent space. </span>
                                     

                         <br>
                    </center>

                    </td>
                </tr>
      
            </table>

          <br>
   <!--        <hr>
            <table align=center width=700>
             <center><h1>News</h1></center>
                <tr>
                <ul>
                <li>[17 July 2020] Paper Accpet</li>
                </ul>
                </tr>
            </table>
         <br>
         <hr> -->
         <!-- <table align=center width=550px> -->
            <table align=center width=700>
             <!--<center><h1>Paper</h1></center>
                <tr>
                  <td><a href=""><img style="height:180px; border: solid; border-radius:30px;" src="./resources/paper.png"/></a></td>

   
                  <td> Yuxuan Zhang *, Huan Ling *, Jun Gao, Kangxue Yin, <br> Jean-Francois Lafleche, Adela Barriuso, Antonio Torralba, Sanja Fidler-->
<br><br>
 <span style="background-color:#00FEFE">  CVPR 2021 (Oral)</span>
             
<!--<br><br> * authors contributed equally<br><br>
                              
                    <span style="font-size:18px">
                      <a href="https://arxiv.org/abs/2104.06490" target="_blank">[Download Paper]</a></
                    <span style="font-size:18px">   <a href="./bib.txt" target="_blank">[Bibtex]</a></span>
                           <span style="font-size:18px">   <a href="https://github.com/nv-tlabs/datasetGAN_release/" target="_blank">[Code and Data]</a></span>-->

          


                    </td>

  

              </tr>
               
                
                
   
              <tr>
                  <td colspan="5" style="font-size: 14px">
    
                  </td>
              </tr>



            </table>
            <br>

            <br>
        <hr>
<!--
 <center><h1>News</h1></center>

<ul><li>
[April 2021] Paper accepted at CVPR 2021 as Oral!
</li>

<li>
[April 2021] Checkout our <a href="https://blogs.nvidia.com/blog/2021/04/16/gan-research-knight-rider-ai-omniverse/">GTC'21 story</a> with ICLR'21 Oral paper  <a href="https://arxiv.org/abs/2010.09125">Ganverse3D</a>      
</li>

</ul>
-->
 <hr>
          <center><h1>Results</h1></center>

          <table align=center width=900px>
              <tr>
                   <center>
                        <a href="./resources/datasetgan1.png"><img src = "./resources/datasetgan1.png" width="900px"></img></a><br>
                    </center>
                       <center>
                        <a href="./resources/datasetgan2.png"><img src = "./resources/datasetgan2.png" width="900px"></img></a><br>
                    </center>
                </tr>
            <tr>
                <center>
                    <span style="font-size:14px">
                         Examples of synthesized images and labels from our DATASETGAN for different classes. Less than 40 annotated images are used for each class. Refer to our paper for details.
                        </span>
                    </center>
            </tr>
          
             <tr>
                   <center>
                        <a href="./resources/datasetgan3.png"><img src = "./resources/datasetgan3.png" width="900px"></img></a><br>
                    </center>
                </tr>
            <tr>
                <center>
                    <span style="font-size:14px">
                          We visualize predictions of DeepLab trained on DATASETGAN's datasets, compared to ground-truth annotations. 
                        </span>
                    </center>
            </tr>
             <br><br>

                  <tr>
                   <center>
                        <a href="./resources/datasetgan4.png"><img src = "./resources/datasetgan4.png" width="900px"></img></a><br>
                    </center>
                </tr>
      

            <tr>
                <center>
                    <span style="font-size:14px">
                     We showcase our detailed part segmentation and keypoint detection in reconstructing animatable 3D objects from monocular images.

                        </span>
                    </center>
            </tr>
            

           
          </table>
 

</body>
</html>

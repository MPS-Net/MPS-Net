<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }

    h1 {
        font-weight:300;
        margin: 0.4em;
    }

    p {
        margin: 0.2em;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
                15px 15px 0 0px #fff, /* The fourth layer */
                15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
                20px 20px 0 0px #fff, /* The fifth layer */
                20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
                25px 25px 0 0px #fff, /* The fifth layer */
                25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr
    {
        margin: 0;
        border: 0;
        height: 1.5px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
</style>

<html>
  <head>
        <title>MPS-Net</title>
  </head>

  <body>
	  <br>
    	<center>
    		<span style="font-size:42px">Capturing Humans in Motion: 3D Human Pose and Shape Estimation Through Motion Continuity Attention and Hierarchical Attentive Feature Integration</span>
    	</center>
    	<br>
	  <br>
	  <br>
            <table align=center width=900px>
		<tr>
	           <td>
                      <center>                      
                           <img src = "./Fig3.png" width="900px" height="410px"></img>
                           <span style="font-size:15px;font-style:italic">   Overview of our motion pose and shape network (MPS-Net). MPS-Net estimates pose, shape, and camera parameters in the
video sequence based on the static feature extractor, temporal encoder, temporal feature integration scheme, and SMPL parameter regressor to generate 3D human pose and shape.</span>
                    </center>
                   </td> 
	        </tr>
            </table>

            <table align=center width=900px></table>
                <tr>
                    <td width=600px>
                        <br>
                        <p align="justify" style="font-size: 18px">
                         Learning to capture human motion is essential to 3D human pose and shape estimation from monocular video. However, the existing methods mainly rely on recurrent or convolutional operation to model such temporal information, which limits the ability to capture non-local context relations of 3D human motion. To address this problem, we propose a motion pose and shape network (MPS-Net) to effectively capture humans in motion to generate accurate and temporally coherent 3D human pose and shape from a video. Specifically, we first propose a motion continuity attention (MoCA) module that leverages visual cues observed from human motion to adaptively recalibrate the
range that needs attention in the sequence to better capture the motion continuity dependencies. We then propose a hierarchical attentive feature integration (HAFI) module to effectively combine past and future feature representations to strengthen temporal correlation and refine the feature representation of the current frame. By jointly considering MoCA and HAFI modules, the proposed MPS-Net excels in estimating 3D human pose and shape in the video. Though conceptually simple, our MPS-Net outperforms the state-of-the-art methods on the challenging 3DPW, MPI-INF-3DHP and Human3.6M benchmark datasets, respectively.
                        </p> 
		    </td>
			
		    <br>
			
                    <center>
                         <td width=600px>
                         	<img src = "./360.gif" width="450px" height="253px"></img>
                         	<img src = "./MPS_Net_DM3-choice_output.gif" width="450px" height="127px"></img><br>
                                <span style="font-size:14px">   Left: The video showcases our MPS-Net under alternative viewpoints. Right: The video shows the visual results of the learned continuity of human motion.  </span>
                          	<br>
			 </td>
                    </center>
                </tr>      
            </table>
<br>
<hr>
            <table align=center width=700>
             <center><h1>Paper</h1></center>
                <tr>
                  <td><a href=""><img style="height:180px; border: solid; border-radius:30px;" src="./paper.png"/></a></td>

   
                  <td> <span style="font-size:18px">                                                                      
<br><br>
 <span style="background-color:#00FEFE"> submitted to CVPR 2022 </span>
             
<br><br>                                  <br><br>
                              
                           <span style="font-size:18px">   <a href="https://github.com/nv-tlabs/datasetGAN_release/" target="_blank">[Code]</a>Coming soon!</span>

          


                    </td>

  

              </tr>
               
                
                
   
              <tr>
                  <td colspan="5" style="font-size: 14px">
    
                  </td>
              </tr>



            </table>


            <br>
 <hr>        
          <table align=center width=900px>
          <center><h1>Demo Videos</h1></center>
            <tr>
                   <center>
                        <!--<a href="./MPS-Net_Kpop_output.gif"><img src = "./MPS-Net_Kpop_output.gif" width="900px"></img></a>-->
                        <a href="./MPS_Net_Kpop_output (1).mp4">
                            <video width="900" controls preload>
                                <source src="./MPS_Net_Kpop_output (1).mp4" type="video/mp4">
                            </video>
                        </a>
                    </center>
                   <center>
                        <!--<a href="./MPS-Net_Kpop_output.gif"><img src = "./MPS-Net_Kpop_output.gif" width="900px"></img></a>-->
                        <a href="./MPS_Net_Kpop_output (2).mp4">
                            <video width="900" controls preload>
                                <source src="./MPS_Net_Kpop_output (2).mp4" type="video/mp4">
                            </video>
                        </a>
                    </center>
                   <center>
                        <!--<a href="./MPS-Net_Kpop_output.gif"><img src = "./MPS-Net_Kpop_output.gif" width="900px"></img></a>-->
                        <a href="./MPS_Net_Kpop_output (4).mp4">
                            <video width="900" controls preload>
                                <source src="./MPS_Net_Kpop_output (4).mp4" type="video/mp4">
                            </video>
                        </a>
                    </center>      
              <td colspan='2'>
                   <center>
                        <a href="./MPS-Net_PrinceRoyce_output.mp4">
                            <video width="450" controls preload>
                                <source src="./MPS-Net_PrinceRoyce_output.mp4" type="video/mp4">
                            </video>
                        </a>
                    </center>      
                </td>
              <td colspan='2'>
                    <center>
                        <a href="./MJ.mp4">
                            <video width="450" controls preload>
                                <source src="./MJ.mp4" type="video/mp4">
                            </video>
                        </a>
                        <br>
                    </center>
                </td>	     
                <td colspan='2'>
                   <center>
                        <a href="./MPS_Net_courtyard_jumpBench_01_output.mp4">
                            <video width="450" controls preload>
                                <source src="./MPS_Net_courtyard_jumpBench_01_output.mp4" type="video/mp4">
                            </video>
                        </a>
                    </center>      
                </td>
              <td colspan='2'>
                    <center>
                        <a href="./MPS_Net_downtown_walkUphill_00_output.mp4">
                            <video width="450" controls preload>
                                <source src="./MPS_Net_downtown_walkUphill_00_output.mp4" type="video/mp4">
                            </video>
                        </a>
                        <br>
                    </center>
                </td>              
            </tr>
            <br>
        </table>
<br>
<hr>            
          <table align=center width=900px>
          <center><h1>Visual effects of MPS-Net under alternative viewpoints</h1></center>
            <tr>
              <th colspan='4'> 
                   <center>
                        <a href="./LSPchoice360.gif"><img src = "./LSPchoice360.gif" width="900px"></img></a><br>
                    <span style="font-size:14px">
                <b>We visualize the 3D human body generated by our MPS-Net from different viewpoints.</b> The results show that our MPS-Net is able to estimate the correct global body rotation.
                      </span>
                    </center>
              </th>
            </tr>
          </table>   
        <br>
 <hr>
          <center><h1>Qualitative Results</h1></center>
             <tr>
                 <th colspan='4'>
                   <center>
                     <span style="font-size:22px">Visual comparisons with the TCMR </span>
                   </center>
                 </th>
             </tr>
<br>
          <table align=center width=900px>
            
                  <tr>
                   <center>
                        <a href="./269.gif"><img src = "./269.gif" width="900px"></img></a><br>
                                        <span style="font-size:14px">
                     Qualitative comparison of TCMR [6] (left) and our MPS-Net (right) on the challenging in-the-wild 3DPW [36] and MPI-INF-3DHP [26] datasets.

                    </span>
                    </center><br>
                   <center>
                        <a href="./Fig6-1.png"><img src = "./Fig6-1.png" width="900px"></img></a><br>
                   </center><br>
                   <center>
                        <a href="./Fig6-2.png"><img src = "./Fig6-2.png" width="900px"></img></a><br>
                   </center><br>
                   <center>
                        <a href="./Fig6-3.png"><img src = "./Fig6-3.png" width="900px"></img></a><br>
                   </center><br>
                   <center>
                        <a href="./Fig6-4.png"><img src = "./Fig6-4.png" width="900px"></img></a><br>
                   </center>
                </tr>              
          </table>
         <br>
        <hr>

        <center><h1> Quantitative Results</h1></center> <br>

        <table align=center width=900px>
            <tr>
                <td>
                    <span style="font-size:18px">
                        <b>Evaluation of state-of-the-art video-based methods on 3DPW [36], MPI-INF-3DHP [26], and Human3.6M [15] datasets.</b> Following Choi et al. [6], all methods are trained on the training set including 3DPW, but do not use the Human3.6M SMPL parameters obtained from Mosh [22]. The number of input frames follows the original protocol of each method.
                        <!--<b>Chest X-ray Lung Segmentation</b> Numbers are DICE scores. 
                        <a href='http://db.jsrt.or.jp/eng.php'>JSRT</a> is the in-domain dataset, 
                        on which we both train and evaluate. We also evaluate on additional out-of-domain datasets 
                        (<a href='https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4256233/'>NLM</a>, 
                        <a href='https://arxiv.org/abs/1803.01199'>NIH</a>, 
                        <a href='https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4256233/'>SZ</a>). 
                        Ours as well as the other semi-supervised methods use additional 108k unlabeled data samples.-->
                    </span>
                </td>                
            </tr>

            <tr>
                <td width=100px>
                    <center>
                        <a href="./Table1.png"><img src = "./Table1.png" width="1000px"></img></a><br>
                    </center>
                </td>
	    </tr>                
        </table>
        <br>
        <table align=center width=900px>
            <tr>
                <td>

                    <span style="font-size:18px">
                        <b>Comparison of the number of network parameters</b>
                        <!--<b>Skin Lesion Segmentation</b> Numbers are JC index. <a href='https://challenge.isic-archive.com/'>ISIC</a>
                        is the in-domain dataset, on which we both train and evaluate. We also evaluate on additional out-of-domain datasets 
                        (<a href='https://pubmed.ncbi.nlm.nih.gov/24110966/'>PH2</a>, 
                        <a href='https://uwaterloo.ca/vision-image-processing-lab/research-demos/skin-cancer-detection'>IS</a>, 
                        <a href='https://uwaterloo.ca/vision-image-processing-lab/research-demos/skin-cancer-detection'>Quest</a>). 
                        Ours as well as the other semi-supervised methods use additional 33k unlabeled data samples.-->
                    </span>
                </td>                
            </tr>

            <tr>
                <td width=300px>
                    <center>
                        <a href="./Table2.png"><img src = "./Table2.png" width="600px"></img></a><br>
                    </center>
                </td>
	    </tr>                
        </table>
        <br>
        <hr>
        <br>
        <table align=center width=900px>
            <tr>
                <td>
                    <span style="font-size:18px">
                        <b>Ablation study for different modules of the proposed MPS-Net on the 3DPW [36] dataset.</b> The training and evaluation settings are the same as he above table.
                        <!--<b>CT-MRI Transfer Liver Segmentation</b> Numbers are DICE per patient. CT
                        is the in-domain data set on which we both train and evaluate. We also evaluate on additional unseen 
                        MRI T1-in and MRI T1-out from
                        <a href='https://chaos.grand-challenge.org/'>CHAOS</a> dataset. 
                        Ours as well as the semi-supervised methods use additional 70 CT volumes from the 
                        <a href='https://competitions.codalab.org/competitions/17094'>LITS2017</a> testing set as unlabeled data samples for training.-->
                    </span>
                </td>                
            </tr>

            <tr>
                <td width=100px>
                    <center>
                        <a href="./Table3.png"><img src = "./Table3.png" width="600px"></img></a><br>
                    </center>
                </td>
            </tr>
        </table>
        <br>
        <hr>
        <br>
        <table align=center width=900px>
            <tr>
                <td>
                    <span style="font-size:18px">
                        <b>Evaluation of state-of-the-art single image-based and video-based methods on 3DPW [36] dataset.</b> All methods do not use 3DPW in training.
                    </span>
                </td>                
            </tr>

            <tr>
                <td width=100px>
                    <center>
                        <a href="./Table4.png"><img src = "./Table4.png" width="600px"></img></a><br>
                    </center>
                </td>
            </tr>
        </table>
        <br>
        <br>
        <hr>
 		  <a name="related_work"></a>
 		  <table align=center width=1100px>
 		    <tr>
 	              <td width=400px>
 					<left>
  		  <center><h1>References</h1></center>

 		  <br><br>

				Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. <b>Generative Adversarial Networks</b>. NIPS, 2014. </br>
                <br>
                Alec Radford, Luke Metz, Soumith Chintala. <b>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</b>. ICLR, 2016. </br>
                <br>
                Jun-Yan Zhu, Philipp Krahenbuhl, Eli Shechtman, Alexei A. Efros. <b>Generative Visual Manipulation on the Natural Image Manifold</b>. ECCV, 2016. </br>
			</td>
		 </tr>
         <tr>
             <td width=400px>
                 <br><br>
                 Jun-Yan Zhu*, Taesung Park*, Phillip Isola, Alexei A. Efros. <b>Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</b>. ICCV, 2017. [<a href="https://arxiv.org/abs/1703.10593">PDF</a>][<a href="https://junyanz.github.io/CycleGAN/">Webpage</a>][<a href="https://github.com/junyanz/CycleGAN">Code</a>]</br>
             </td>
         </tr>

	 </table>
	  <br>
	  <hr>
  </body>
</html>
